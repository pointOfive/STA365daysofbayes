{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b477d3e7",
   "metadata": {},
   "source": [
    "# Metropolis Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43bdb3",
   "metadata": {},
   "source": [
    "## Part A: MH Proposals and Acceptance Probabilities\n",
    "\n",
    "> *Starting with a very general notation...*\n",
    "\n",
    "Accepting draws from a ***proposal distribution*** \n",
    "\n",
    "$\\tilde x^{(t)} \\sim q(\\tilde x^{(t)}|x^{(t-1)})$\n",
    "\n",
    "according to \n",
    "\n",
    "$x^{(t)} = \\left\\{\\begin{array}{ll}\n",
    "\\tilde x^{(t)} & \\text{with probability } \\min\\left(1,\\frac{p(\\tilde x^{(t)})}{p(x^{(t-1)})}\\frac{q(x^{(t-1)}|\\tilde x^{(t)})}{q(\\tilde x^{(t)}|x^{(t-1)})}\\right) \\quad \\color{gray}{\\begin{array}{c}\\textrm{... is the normalizing constant needed for $p$?}\\\\\\textrm{... and what happens if $q$ is symmetric?}  \\end{array}}\\\\\n",
    "x^{(t-1)} & \\text{otherwise}\n",
    "\\end{array}\\right.$\n",
    "\n",
    "will produce draws from $p(x^{(t)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab0bd2",
   "metadata": {},
   "source": [
    "### This is a two step conditional process \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "&\\quad\\, \\Pr(\\textrm{accept}|\\textrm{proposal},\\textrm{current state}) p(\\textrm{proposal}|\\textrm{current state})\\\\ \n",
    "&={}\n",
    "\\Pr(\\textrm{accept}|\\textrm{proposal}) p(\\textrm{proposal}|\\textrm{current state})\\\\\n",
    "&={} \\Pr(x^{(t)} = \\tilde x^{(t)}) q(\\tilde x^{(t)}|x^{(t-1)})\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### Does it work?\n",
    "\n",
    "Let's see for \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p(x^{(t)}) & \\rightarrow {} N(0,1)\\\\\n",
    "q(\\tilde x^{(t)}|x^{(t-1)}) & \\rightarrow {} N(x^{(t-1)},\\sigma)\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_x_pdf = lambda x: stats.norm().pdf(x)\n",
    "q_xt_given_xtm1 = lambda x: stats.norm(x, scale=0.1)\n",
    "proposal_distribution = q_xt_given_xtm1\n",
    "\n",
    "m = 1000\n",
    "x = np.zeros(m)\n",
    "rejections = 0\n",
    "for t in range(1,m):\n",
    "    x_tilde = proposal_distribution(x[t-1]).rvs()\n",
    "    acceptance_probability = min(1, p_x_pdf(x_tilde)/p_x_pdf(x[t-1]))\n",
    "    if stats.uniform().rvs() < acceptance_probability:\n",
    "        x[t] = x_tilde\n",
    "    else:\n",
    "        x[t] = x[t-1]\n",
    "        rejections += 1\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].plot(x)\n",
    "ax[1].hist(x, density=True)\n",
    "x_support = np.linspace(-4,4,301)\n",
    "ax[1].plot(x_support, p_x_pdf(x_support));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75981515",
   "metadata": {},
   "source": [
    "## Part B: What is MH?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e87d11",
   "metadata": {},
   "source": [
    "### Why doesn't this work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed116ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 300\n",
    "autocorrelations = np.ones(K)\n",
    "for t_plus_k in range(1, K):\n",
    "    autocorrelations[t_plus_k] = np.corrcoef(x[:-t_plus_k], x[t_plus_k:])[0,1]\n",
    "    \n",
    "plt.plot(autocorrelations);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29feda19",
   "metadata": {},
   "source": [
    "### Effective sample size\n",
    "\n",
    "$$n_{\\text{eff}} = \\frac{m}{\\sum_{k=-\\infty}^\\infty\\rho_k} = \\frac{m}{1+2\\sum_{k=1}^\\infty\\rho_k} \\quad \\text{ for order $k$ autocorrelations } \\quad \\rho_k $$\n",
    "\n",
    "which is based on the relative value of $m$ between\n",
    "- $\\textrm{Var}\\left({\\sum_{t=1}^m x^{(t)}}\\right) = m \\sigma^2$ under i.i.d. sampling and the actual \n",
    "- $\\textrm{Var}\\left(\\sum_{t=1}^m x^{(t)}\\right) = \\sum_{t=1}^m \\textrm{Var}\\left(x^{(t)}\\right) + \\sum_{t \\neq t'} \\textrm{Cov}(x^{(t)},x^{(t')}) = m \\sigma^2 + \\sum_{t \\neq t'} \\sigma^2 \\rho_{t,t'} \\approx m \\sigma^2+2m \\sigma^2 \\sum_{k=1}^m\\rho_k \\approx m \\sigma^2+2m \\sigma^2 \\sum_{k=1}^\\infty\\rho_k $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32875ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m / (1 + 2*autocorrelations[:100].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 300\n",
    "autocorrelations = np.ones(K)\n",
    "for t_plus_k in range(1, K):\n",
    "    autocorrelations[t_plus_k] = np.corrcoef(x[:-t_plus_k], x[t_plus_k:])[0,1]\n",
    "\n",
    "approximation_stop = (autocorrelations < 0).cumsum()\n",
    "approximation_stop = approximation_stop*np.arange(K)\n",
    "approximation_stop = min(approximation_stop[approximation_stop>0])\n",
    "\n",
    "plt.plot(autocorrelations) \n",
    "plt.vlines(approximation_stop, ymin=0, ymax=1, color='k',\n",
    "           label='higher order correlations\\nafter this value will\\neventually cancel out')\n",
    "plt.legend()\n",
    "# effective sample size\n",
    "m / (1 + 2*autocorrelations[:approximation_stop].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185db47",
   "metadata": {},
   "source": [
    "### Rejection Rate\n",
    "\n",
    "The **rejection rate** of the **Metropolis Hastings** algorithm given above is something else that we should keep in mind.\n",
    "\n",
    "#### Q1: Is a high rejection rate benefiicial or detrimental for the effective sample size calculation?\n",
    "\n",
    "#### Q2: What is the relationship between the proposal distribution and the rejection rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejections/m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2936f3bb",
   "metadata": {},
   "source": [
    "### Why does this work?\n",
    "\n",
    "> *Changing and hinting with the the notation to begin being suggestive about our purposes...*\n",
    "\n",
    "If we have a **stationary distribution** $p(\\theta \\color{gray}{|x})$ then **transition distribution (kernel)** $q$ produces a sample from this **stationary distribution** if the ***Markov chain*** it produces is ***reversible***.\n",
    "\n",
    "$\\require{cancel}\n",
    "\\begin{align*}\n",
    "p(\\theta^{(t)}|\\theta^{(t-1)})p(\\theta^{(t-1)}\\color{gray}{|x}) & = {}  p(\\theta^{(t-1)}|\\theta^{(t)})p(\\theta^{(t)}\\color{gray}{|x})\\\\\\\\\n",
    "& \\textrm{But is this true if we use a Metropolis-Hastings transition distribution (kernel)?}\n",
    "\\\\\\\\\n",
    "p(\\theta^{(t)}\\color{gray}{|x}) & \\overset{MH}{=} {} \\frac{\\alpha_{\\textrm{accept prob}}^{\\textrm{forward}} q(  \\theta^{(t)}| \\theta^{(t-1)})}{\\alpha_{\\textrm{accept prob}}^{\\textrm{backward}} q(  \\theta^{(t-1)}|\\theta^{(t)})}p(\\theta^{(t-1)}\\color{gray}{|x})\\\\\n",
    "p( \\theta^{(t)}\\color{gray}{|x}) & = {} \\frac{\\min\\left(1,\\frac{p( \\theta^{(t)}\\color{gray}{|x})}{p(\\theta^{(t-1)}\\color{gray}{|x})}\\frac{q(\\theta^{(t-1)}| \\theta^{(t)})}{q( \\theta^{(t)}|\\theta^{(t-1)})}\\right) q( \\theta^{(t)}|\\theta^{(t-1)})}{\\min\\left(1,\\frac{p( \\theta^{(t-1)}\\color{gray}{|x})}{p( \\theta^{(t)}\\color{gray}{|x})}\\frac{q( \\theta^{(t)}| \\theta^{(t-1)})}{q( \\theta^{(t-1)}| \\theta^{(t)})}\\right) q(  \\theta^{(t-1)}|\\theta^{(t)})}p(\\theta^{(t-1)}\\color{gray}{|x})\\\\\\\\\n",
    "& \\textrm{The fractions in the \"$\\min$\" in the numerator or denomenator are reciprocals}\\\\\n",
    "& \\textrm{thus regardless of if the numerator fraction is equal to, greater than, or less than $1$}\\\\\\\\\n",
    "p( \\theta^{(t)}\\color{gray}{|x}) & = {} \\frac{p( \\theta^{(t)}\\color{gray}{|x})}{p(\\theta^{(t-1)}\\color{gray}{|x})}\\frac{q(\\theta^{(t-1)}| \\theta^{(t)})}{q( \\theta^{(t)}|\\theta^{(t-1)})} \\frac{q\\theta^{(t)}|\\theta^{(t-1)})}{q(\\theta^{(t-1)}|\\theta^{(t)})}p(\\theta^{(t-1)}\\color{gray}{|x}) \\\\\\\\\n",
    "\\xcancel{p( \\theta^{(t)}\\color{gray}{|x})} & = {} \\frac{\\xcancel{p(\\theta^{(t)}\\color{gray}{|x})}}{\\xcancel{p(\\theta^{(t-1)}\\color{gray}{|x})}}\\frac{\\cancel{q(\\theta^{(t-1)}| \\theta^{(t)})}}{\\cancel{q( \\theta^{(t)}|\\theta^{(t-1)})}} \\frac{\\cancel{q\\theta^{(t)}|\\theta^{(t-1)})}}{\\cancel{q(\\theta^{(t-1)}|\\theta^{(t)})}}\\xcancel{p(\\theta^{(t-1)}\\color{gray}{|x})}\\\\  \\longrightarrow \\quad 1 & ={} 1 \\quad \\textrm{ so the equation is true and reversibility holds}\n",
    "\\end{align*}$\n",
    "\n",
    "So **Metropolis-Hastings** creates a **reversible Markov Chain** whose **stationary distribution** is the **target** of the **Metropolis-Hastings** algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f0e7a",
   "metadata": {},
   "source": [
    "## Part C: Back to Gibbs sampling\n",
    "\n",
    "**Gibbs sampling** also creates a **reversible Markov Chain** whereby the samples, forwards or backwards, are from the same **stationary distribution** (which is the **joint posterior distribution** of the (random. variable) parameters of the **full conditional (posterior) distributions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc is mean and scale is standard deviation\n",
    "n = 30; x = stats.norm(loc=0, scale=1).rvs(size=n)\n",
    "C,G = 2,10000  # chains, Gibbs samples\n",
    "theta,tau = np.zeros([C,G]),np.zeros([C,G])\n",
    "theta[:,0] = 1000  # initialization\n",
    "for c in range(C):\n",
    "    for g in range(1,G):\n",
    "        # https://en.wikipedia.org/wiki/Gamma_distribution\n",
    "        tau[c,g] = \\\n",
    "        stats.gamma(a=n/2+1, \n",
    "                    scale=2/((x-theta[c,g-1])**2).sum()).rvs()\n",
    "        theta[c,g] = \\\n",
    "        stats.norm(x.mean(), \n",
    "                   scale=1/np.sqrt(tau[c,g]*n)).rvs()\n",
    "\n",
    "fig,ax = plt.subplots(1, 4, figsize=(14,3)); ax[0].set_title(\"$\\\\theta$ Burn-In\"); ax[1].set_title(\"$\\\\tau$ Burn-In\"); ax[2].set_title(\"$\\\\theta$ Converged Markov Chain\\nStationary Distribution Samples\"); ax[3].set_title(\"$\\\\tau$ Converged Markov Chain\\nStationary Distribution Samples\")\n",
    "burn = 20\n",
    "demo = 120\n",
    "for c in range(C):\n",
    "    ax[0].plot(theta[c,:burn], label=\"$\\\\theta$ Chain \"+str(c))\n",
    "    ax[1].plot(tau[c,:burn], label=\"$\\\\tau$ Chain \"+str(c))\n",
    "    ax[2].plot(np.arange(burn, demo, dtype=int), theta[c,burn:demo], label=\"$\\\\theta$ Chain \"+str(c))\n",
    "    ax[3].plot(np.arange(burn, demo, dtype=int), tau[c,burn:demo], label=\"$\\\\tau$ Chain \"+str(c))\n",
    "ax[0].legend(); ax[1].legend(); ax[2].legend(); ax[3].legend();         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53300dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 3, figsize=(16,4))\n",
    "ax[0].set_title(\"Marginal distribution of $\\\\theta$ (without burn-in)\\nof (joint posterior) stationary distributions\")\n",
    "ax[1].set_title(\"Marginal distribution of $\\\\tau$ (without burn-in)\\nof (joint posterior) stationary distributions\")\n",
    "ax[2].set_title(\"Joint (posterior) stationary distributions\\n(without burn-in)\")\n",
    "\n",
    "ax[0].hist(theta[0,burn:])\n",
    "ax[1].hist(tau[0,burn:])\n",
    "ax[2].plot(theta[0,burn:], tau[0,burn:], '.', alpha=0.1);\n",
    "demo = 3*burn\n",
    "ax[2].plot(theta[0,burn:demo], tau[0,burn:demo]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf8ba4",
   "metadata": {},
   "source": [
    "## Metropolis within Gibbs\n",
    "\n",
    "1. All **full conditional distributions** are proportional to the **joint**.\n",
    "2. A **Metropolis-Hastings** step can sample from a **full conditional** by proposing a sample and accepting it according to the MH acceptance probability.\n",
    "3. The target density in the MH acceptance probability appears in the numerator and denomenator so it's normalizing constants cancel and all that's needed \n",
    "\n",
    "Accepting draws from a ***proposal distribution*** \n",
    "\n",
    "$\\tilde \\theta^{(t)} \\sim q(\\tilde \\theta^{(t)}|\\theta^{(t-1)})$\n",
    "\n",
    "according to \n",
    "\n",
    "$\\require{cancel}\n",
    "\\theta^{(t)} = \\left\\{\\begin{array}{ll}\n",
    "\\tilde \\theta^{(t)} & \\text{with probability } \\min\\left(1,\\frac{p(\\tilde \\theta^{(t)}|\\tau^{(t)},\\mathbf{x})}{p(\\theta^{(t-1)}|\\tau^{(t)},\\mathbf{x})}\\frac{q(\\theta^{(t-1)}|\\tilde \\theta^{(t)})}{q(\\tilde \\theta^{(t)}|\\theta^{(t-1)})}\\right) = \\min\\bigg(1,\\frac{p(\\tilde \\theta^{(t)},\\tau^{(t)},\\mathbf{x})}{p(\\theta^{(t-1)},\\tau^{(t)},\\mathbf{x})}\\overset{\\textrm{if symmetric}}{\\cancel{\\frac{q(\\theta^{(t-1)}|\\tilde \\theta^{(t)})}{q(\\tilde \\theta^{(t)}|\\theta^{(t-1)})}}^1}\\bigg) \\\\\n",
    "\\theta^{(t-1)} & \\text{otherwise}\\\\\\\\\n",
    "\\end{array}\\right.$\n",
    "\n",
    "will produce draws from $p(\\theta^{(t)}|\\tau^{(t)},\\mathbf{x})$.\n",
    "\n",
    "And an analogous **Metropolis within Gibbs** step exists for $\\tau^{(t)}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71b629",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "---\n",
    "\n",
    "### Q1\n",
    "\n",
    "Convert your **Gibbs sampler** from the previous homework to be a **Metropolis within Gibbs** sampler. \n",
    "- Do not derive any full conditionals and instead just use the joint distribution which is the product of the likelihood with the priors \n",
    "    - Note that in fact you must only be proportional to the joint since irrelevant priors of \"the other parameter\" will cancel in the ratio\n",
    "- Note that $\\tau$ must be positive so proposals $\\tilde \\tau$ must also be positive. The easiest way to achieve this would be to use independent samples (that do not depend on $\\tau^{(t-1)}$ from exponential, half normal, gamma, or inverse gamma distributions (and other distributions defining strictly positive random variables).\n",
    "- When considering using independent proposal distributions one option of some note is to use the prior $p(\\tau)$ as this proposal distribution\n",
    "\n",
    "*Clearly articulate your specification and provide a demonstration showing the Bayesian inference it provides, in contrast to that of your previous Gibbs sampler (of Q2 from the last homework).*\n",
    "\n",
    "### Q2\n",
    "\n",
    "Adjust your **Metropolis within Gibbs** sampler as follows.\n",
    "\n",
    "- Use the following **dependent** proposal for $\\tau$\n",
    "\n",
    "```Python\n",
    "stats.truncnorm(a=-tau_t_minus_1/s, b=np.Inf,\n",
    "                loc=tau_t_minus_1, scale=s)\n",
    "```\n",
    "\n",
    "- Use a prior for $\\theta$ that is not a normal distribution \n",
    "\n",
    "\n",
    "*Provide a demonstration showing the Bayesian inference provided by this sampler. Then discuss the necessity (or lackthereof)  of analytically determining the full conditional distributions when using Metropolis-Hastings.*\n",
    "\n",
    "### Q3 \n",
    "\n",
    "For **Metropolis Hastings** targetting (**stationary distribution**) $p$ based on proposal distribuiton $q$ given below\n",
    "\n",
    "$\\begin{align}\n",
    "q(\\tilde x^{(t)} | x^{(t-1)}) &={} \\left(\\frac{1}{2}\\right)^{1-{\\tilde x^{(t)}}}\\left(\\frac{1}{2}\\right)^{\\tilde x^{(t)}} \\quad \\color{gray}{\\textrm{ ... does } \\quad \\frac{q(x^{(t-1)}|\\tilde x^{(t)})}{q(\\tilde x^{(t)}|x^{(t-1)})} \\quad  \\textrm{ cancel?}}\\\\\n",
    "\\textrm{and} \\quad p(x^{(t)}) &={} \\left(\\frac{1}{3}\\right)^{1-{x^{(t)}}}\\left(\\frac{2}{3}\\right)^{x^{(t)}}\\\\\n",
    "\\end{align}$\n",
    "\n",
    "<u>show that</u> the **transition kernel** $K$ of transition probabilities is defined the **Metropolis Hastings** algorithm is\n",
    "\n",
    "$K = \\left[\\begin{array}{cc}\\Pr(x^{(t)}=0 \\rightarrow x^{(t+1)}=0) & \\Pr(x^{(t)}=1 \\rightarrow x^{(t+1)}=0)\\\\\\Pr(x^{(t)}=0 \\rightarrow x^{(t+1)}=1)& \\Pr(x^{(t)}=1 \\rightarrow x^{(t+1)}=1)\\end{array}\\right] = \\left[\\begin{array}{cl} 0.5 & 0.25 \\\\ 0.5 & 0.75 \\end{array}\\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372db60",
   "metadata": {},
   "source": [
    "This means that\n",
    "\n",
    "- if we're currently a $0$ there's a 50\\% chance that we'll stay a $0$ and 50\\% chance that we'll change to a $1$\n",
    "- and if we're currently a $1$ there's a 75\\% chance that we'll stay a $1$ and 25\\% chance that we'll change to a $0$\n",
    "\n",
    "which specifies [geometric distributions](https://en.wikipedia.org/wiki/Geometric_distribution) with, respectively, expected values of $1$ and $3$ \"failures before transitioning to the other state\" which means that on average a \"$0$ state\" will be a run of two $0$'s while a \"$1$ state\" will be a run of four $1$'s which leads to the desired one-to-two (or one-thirds to two-thirds) ratio of $0$'s and $1$ is the Markov Chain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c81894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's what it looks like if we flip coins  \n",
    "# with transition probabilties defind by A sequentially\n",
    "# and sure enough we recover 1/3 and 2/3 for 0 and 1 respectively\n",
    "K = np.array([[.5,.25],[.5,.75]]) \n",
    "T = 200\n",
    "s_t = np.zeros(T, dtype=int)\n",
    "for t in range(1, T):\n",
    "    s_t[t] = int(stats.uniform().rvs() < K[:,s_t[t-1]][1])\n",
    "    \n",
    "fig,ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "ax[0].plot(s_t)\n",
    "ax[1].hist(s_t, bins=2, density=True)\n",
    "ax[1].set_title(\"Proportion of 1's here are \"+str(s_t.mean()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's what it looks like if we flip coins  \n",
    "# with transition probabilties defind by K \n",
    "# and sure enough we recover 1/3 and 2/3 for 0 and 1 respectively\n",
    "T = 10000\n",
    "s_t = np.zeros(T, dtype=int)\n",
    "for t in range(1, T):\n",
    "    s_t[t] = int(stats.uniform().rvs() < K[:,s_t[t-1]][1])\n",
    "    \n",
    "state0_run_length = []\n",
    "state1_run_length = []\n",
    "run_length = 1\n",
    "for t in range(1,T):\n",
    "    if s_t[t]==s_t[t-1]:\n",
    "        run_length += 1\n",
    "    else:\n",
    "        if s_t[t-1] == 0:\n",
    "            state0_run_length += [run_length]\n",
    "        else:\n",
    "            state1_run_length += [run_length]        \n",
    "        run_length = 1\n",
    "        \n",
    "fig,ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "ax[0].hist(state0_run_length)\n",
    "ax[0].set_title(\"0 run lengths on aveage here are \"+str(np.mean(state0_run_length).round(3)))\n",
    "ax[1].hist(state1_run_length)\n",
    "ax[1].set_title(\"1 run lengths on aveage here are \"+str(np.mean(state1_run_length).round(3)));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f312e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
