{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a027bd2b",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo<br><sub>Not quite NUTS but just unhinged enough to give you the right ideas</sub><!-- https://www.johndcook.com/t_normal_mixture.pdf -->\n",
    "\n",
    "<!--\n",
    "$\\begin{align}\n",
    "\\frac{1}{\\sqrt{2\\pi}} \\frac {\\frac{\\nu}{2}^{\\frac{\\nu}{2}}}{\\Gamma \\left(\\frac{\\nu}{2}\\right)}  \\int  e^{-\\frac{1}{2}\\tau_i(y_i^2 + \\nu) } \\tau_i^{\\frac{\\nu+1}{2}-1} d\\tau_i &={} \\frac{1}{\\sqrt{2\\pi}} \\frac{\\sqrt{\\frac{\\nu}{2}}}{\\sqrt{\\frac{\\nu}{2}}} \\frac {\\frac{\\nu}{2}^{\\frac{\\nu}{2}}}{\\Gamma \\left(\\frac{\\nu}{2}\\right)} \\Gamma \\left(\\frac{\\nu+1}{2}\\right) \\left(\\frac{y_i^2 + \\nu}{2}\\right)^{-\\frac{\\nu+1}{2}} \\\\\n",
    "&={} \\frac{1}{\\sqrt{\\pi\\nu}}  \\frac {\\frac{\\nu}{2}^{\\frac{\\nu+1}{2}}}{\\Gamma \\left(\\frac{\\nu}{2}\\right)} \\Gamma \\left(\\frac{\\nu+1}{2}\\right) \\left(\\frac{y_i^2 + \\nu}{2}\\right)^{-\\frac{\\nu+1}{2}}\\\\\n",
    "&={} \\frac{1}{\\sqrt{\\pi\\nu}}  \\frac{\\Gamma \\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma \\left(\\frac{\\nu}{2}\\right)} \\frac{\\nu}{2}^{\\frac{\\nu+1}{2}} \\left(\\frac{y_i^2 + \\nu}{2}\\right)^{-\\frac{\\nu+1}{2}}\\\\\n",
    "&={} \\frac{1}{\\sqrt{\\pi\\nu}}  \\frac{\\Gamma \\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma \\left(\\frac{\\nu}{2}\\right)} \\left(\\frac{y_i^2}{\\nu} + 1\\right)^{-\\frac{\\nu+1}{2}}\n",
    "\\end{align}$\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da21f5",
   "metadata": {},
   "source": [
    "## Hamiltonian Dynamics\n",
    "\n",
    "Consider the **negative log joint probability distribution** for random varialbes $\\theta$ and $v$ with $\\dim(v)=\\dim(\\theta)$ \n",
    "\n",
    "$$\\Large\n",
    "\\begin{align}\n",
    "- \\log p(\\theta, v) = {} & - \\log p(\\theta) p(v|\\theta)   \\\\\n",
    " = {} & - \\log p(\\theta) - \\log p(v|\\theta)   \\\\\n",
    "H(\\theta, v) = {} & U(\\theta) + K(v|\\theta) \\\\\n",
    "\\text{often simplified to } \\Longrightarrow \\quad = {} & U(\\theta) + K(v) \\quad \\textrm{ by assuming } \\quad  \\theta \\perp\\!\\!\\!\\perp v\n",
    "\\end{align}$$\n",
    "\n",
    "> You care about sampling value from $\\theta$ but $v$ is a completely **auxilliary variable** that you just made up and augmented $\\theta$ through $K(v|\\theta)$ which which probably just independent $K(v)$\n",
    "\n",
    "#### <u>Physics people:</u>\n",
    "\n",
    "This will define a **hamiltonian dynamics system** which decomposes the **total energy** $H(\\theta, v)$ into \n",
    "- **potential energy** $U(\\theta)$ and \n",
    "- **kinetic energy** $K(v)$, which is often called **momentum** or **velocity**\n",
    "\n",
    "if it is defined to evolve according to differential equations \n",
    "\n",
    "$$\\Large \\begin{align*} \n",
    "\\frac{d\\theta}{dt} & = {} \\frac{dH}{dv} & \\frac{dv}{dt} & = {}  -\\frac{dH}{d\\theta}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "which induce the **law of conservation of energy**\n",
    "\n",
    "\n",
    "#### <u>Statistics people:</u>\n",
    "\n",
    "The **energies** $\\theta_0 \\overset{time}{\\rightarrow} \\theta_t$ and $v_0 \\overset{time}{\\rightarrow} v_t$ evolve over time but the **total energy**, defined to be the log joint density $\\log p(\\theta_0, v_0\\color{gray}{|x}) = \\log p(\\theta_t, v_t\\color{gray}{|x})$ remains constant\n",
    "\n",
    "#### <u>Physics people:</u>\n",
    "\n",
    "The variables $\\theta$ and $v$ are evolved over time so the **potential** $U(\\theta)$ and **kinetic** $K(v)$ **energy** are always inversely synchronized and the **total energy** remains constant\n",
    "\n",
    "#### <u>Statistics people:</u>\n",
    "\n",
    "You're walking on the countours of a distribution\n",
    "\n",
    "- [Radford Neal \"MCMC using Hamiltonian dynamics\"](https://arxiv.org/pdf/1206.1901)\n",
    "- [Michael Betancourt \"A Conceptual Introduction to Hamiltonian Monte Carlo\"](https://arxiv.org/pdf/1701.02434)\n",
    "- [Alex Rogozhnikov \"Hamiltonian Monte Carlo explained\"](https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af989db4",
   "metadata": {},
   "source": [
    "### Normal Energy\n",
    "\n",
    "If $p(\\theta, v)$ is an **isotropic bivariate normal distribution** then \n",
    "\n",
    "$$H(\\theta, v) = \\log p(\\theta, v) \\overset{\\large \\theta \\perp\\!\\!\\!\\perp v}{=} \\log[p(\\theta)q(v)] = \\log p(\\theta) + \\log q(v) = U(\\theta) + K(v) = \\frac{1}{2}\\theta^2 +\\frac{1}{2}v^2$$\n",
    "\n",
    "and **conservation of energy** $U(\\theta) + K(v)$ is achieved if \n",
    "\n",
    "\\begin{align} \\frac{d\\theta}{dt} & = {} \\frac{dH}{dv} & \\frac{dv}{dt} & = {}  -\\frac{dH}{d\\theta} \\\\ & = {} \\frac{dK}{dv} = v & & = {}   -\\frac{dU}{d\\theta}= -\\theta  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba112f",
   "metadata": {},
   "source": [
    "## Differential Equations\n",
    "\n",
    "The paths of $\\theta$ and $v$ which which satisfy these differential equations have the form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_t &={} r \\cos(a + t)  & v_t  &={} -r \\sin(a + t)\\\\\n",
    "\\frac{d}{dt}\\theta_t  &={} -r \\sin(a + t)   & \\frac{d}{dt} v_t  &={} -r \\cos(a + t)\\\\\n",
    " &={} v_t &  &={} - \\theta_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and they evolve $\\theta_t$ and $v_t$ on the contours of a unit circle (which are the contours of an isotropic bivariate normal distribution).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "r,a = 1,0 \n",
    "t = np.linspace(0,10,1000)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9,4));\n",
    "\n",
    "ax[0].plot(r*np.cos(a+t), label='$\\\\theta_t$')\n",
    "ax[0].plot(-r*np.sin(a+t), label='$v_t$')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.cos(t), -np.sin(t), 'k')\n",
    "ax[1].set_xlabel('$\\\\theta_t = \\\\cos(t)$')\n",
    "ax[1].set_ylabel('$v_t = -\\\\sin(t)$')\n",
    "ax[1].set_aspect('equal', adjustable='box') \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a53a3",
   "metadata": {},
   "source": [
    "## Symplectic Integration<br>(discretely approximated differential equations)\n",
    "\n",
    "\n",
    "> These processes are referred to as integration because they compute a summation series just like how Riemann sums add up areas under a curve (or how Monte Carlo integration estimates this by averaging up \"randomly sampled points\" of the curves height). Indeed, solutions to differential equations must be the integrals of the functions which when differentiated solve the equations. \n",
    "\n",
    "\n",
    "The following numerically approximate the solution trajectory of differential equations.\n",
    "\n",
    "1. ***Euler's method***: not a ***symplectic integrator***\n",
    "\n",
    "$$\\begin{align} \n",
    "\\theta_{t+\\epsilon} & = {} \\theta_t + \\epsilon \\frac{d\\theta_t}{d t} &\n",
    "v_{t+\\epsilon} & = {} v_t + \\epsilon \\frac{dv_t}{d t}\\\\\n",
    "& = {} \\theta_t + \\epsilon v_t &\n",
    "& = {} v_t - \\epsilon \\theta_t\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "2. ***Euler's method*** (***improved variant***): a ***first order symplectic integrator***\n",
    "\n",
    "$$\\begin{align} \n",
    "\\theta_{t+\\epsilon} & = {} \\theta_t + \\epsilon \\frac{d\\theta_t}{d t} & \n",
    "v_{t+\\epsilon} & = {} v_t + \\epsilon \\frac{dv_{t+\\epsilon}}{d t}\\\\\n",
    "& = {} \\theta_t + \\epsilon v_t &\n",
    "& = {} v_t - \\epsilon \\theta_{t+\\epsilon}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "3. The ***leapfrog method*** (or ***leapfrog integration***): a  ***second order symplectic integrator***\n",
    "\n",
    "$$\\begin{align} \n",
    "\\theta(t+\\epsilon/2) & = {} \\theta_t + \\frac{\\epsilon}{2} \\frac{d\\theta_t}{d t} &\n",
    "v_{t+\\epsilon} & = {} v_t + \\epsilon \\frac{dv_{t+\\epsilon/2}}{d t} &\n",
    "\\theta_{t+\\epsilon} & = {} \\theta(t + \\epsilon/2) + \\frac{\\epsilon}{2} \\frac{d\\theta_{t + \\epsilon}}{d t}\\\\\n",
    "& = {} \\theta_t + \\frac{\\epsilon}{2} v_t&\n",
    "& = {} v_t - \\epsilon \\theta_{t+\\epsilon/2}&\n",
    "& = {} \\theta(t + \\epsilon/2) + \\frac{\\epsilon}{2} v_{t+\\epsilon}\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,2*np.pi, 100)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,8)) \n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].plot(np.cos(t), -np.sin(t), 'k')\n",
    "    ax[i].set_aspect('equal', adjustable='box')\n",
    "    \n",
    "ax[0].set_title(\"Euler's Method (not Symplectic)\")\n",
    "theta_t, v_t, t, epsilon = 0, 1, 0, .3\n",
    "for i in range(20):\n",
    "    v_t_e = v_t - epsilon * theta_t\n",
    "    theta_t_e = theta_t + epsilon * v_t\n",
    "    ax[0].plot([theta_t, theta_t_e], [v_t, v_t_e], linewidth=5) \n",
    "    theta_t, v_t, t = theta_t_e, v_t_e, t+epsilon \n",
    "\n",
    "ax[1].set_title(\"Modified Euler's (First Order Symplectic)\")\n",
    "theta_t, v_t, t, epsilon = 0, 1, 0, .75\n",
    "for i in range(50):\n",
    "    v_t_e = v_t - epsilon * theta_t\n",
    "    theta_t_e = theta_t + epsilon * v_t_e\n",
    "    ax[1].plot([theta_t, theta_t_e], [v_t, v_t_e]) \n",
    "    theta_t, v_t, t = theta_t_e, v_t_e, t+epsilon\n",
    "    \n",
    "ax[2].set_title(\"The Leapfrog Method (Second Order Symplectic)\")\n",
    "theta_t, v_t, t, epsilon = 0, 1, 0, .1\n",
    "for i in range(50):\n",
    "    v_t_e = v_t - epsilon / 2 * theta_t\n",
    "    theta_t_e = theta_t + epsilon * v_t_e\n",
    "    v_t_e = v_t_e - epsilon / 2 * theta_t_e\n",
    "    ax[2].plot([theta_t, theta_t_e], [v_t, v_t_e], linewidth=10) \n",
    "    theta_t, v_t, t = theta_t_e, v_t_e, t+epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0999783",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo<br><sub>Remember Metropolis-Hastings...</sub>\n",
    "\n",
    "\n",
    "To produce draws from $p(x^{(t)})$ samples from a **proposal distribution** $\\tilde x^{(t)} \\sim q(\\tilde x^{(t)}|x^{(t-1)})$ and \"flip a coin\" to accept or reject the proposal according to \n",
    "\n",
    "$x^{(t)} = \\left\\{\\begin{array}{ll}\n",
    "\\tilde x^{(t)} & \\text{with probability } \\min\\left(1,\\frac{p(\\tilde x^{(t)})}{p(x^{(t-1)})}\\frac{q(x^{(t-1)}|\\tilde x^{(t)})}{q(\\tilde x^{(t)}|x^{(t-1)})}\\right) \\quad \\color{gray}{\\begin{array}{c}\\textrm{... is the normalizing constant needed for $p$?}\\\\\\textrm{... and what happens if $q$ is symmetric?}  \\end{array}}\\\\\n",
    "x^{(t-1)} & \\text{otherwise}\n",
    "\\end{array}\\right.$\n",
    "\n",
    "---\n",
    "\n",
    "1. Randomly sampling a number of discrete approximation steps (from some discrete distribution you choose)\n",
    "\n",
    "\n",
    "2. Evolve to $\\theta_t$ and $v_t$ with **symplectic integration** to create a **Metropolis-Hastings** proposal distribution $\\tilde p( \\theta_t,  v_t|\\theta_0,v_0)$ with an **aceptance probability** of $1$\n",
    "\n",
    "  $$\\require{cancel}\\min\\left(1,\\cancel{\\frac{p( \\theta_t,  v_t|x)}{p(\\theta_0,v_0|x)}}^1\\cancel{\\frac{ \\tilde p(\\theta_0,v_0| \\theta_t,  v_t)}{\\tilde p( \\theta_t,  v_t|\\theta_0,v_0)}}^1\\right)$$ \n",
    "\n",
    "  assuming \n",
    "  1. proposals $ \\theta_t$ and $ v_t$ are on the same **contour** of the **log joint probability distribution** as initial values $\\theta_0$ and $v_0$ (so the first ratio cancels) \n",
    "  2. and the symplectic integration is **reversibly symmetric** (so the second ratio cancels)\n",
    "  \n",
    "  <u>*Pause: do not yet accept this transition.*</u>\n",
    "\n",
    "\n",
    "3. **Symmetrically randomly** perturb the **momentum** $v_t$ to $v_t+\\epsilon$ to incease or decrease the overall **energy** up or down to higher or lower levels of the contours of the **log joint probability distribution** and accept now accept the full **Metropolis-Hastings** proposal with an **aceptance probability** of \n",
    "\n",
    "   $$\\require{cancel}\\min\\left(1,{\\frac{p( \\theta_t, v_t+\\epsilon|x)}{p(\\theta_t,v_t|x)}}\\cancel{\\frac{\\tilde q(\\theta_t,v_t)|\\theta_t,v_t+\\epsilon)}{\\tilde q(\\theta_t,v_t+\\epsilon| \\theta_t,  v_t)}}^1 \\cancel{\\frac{ \\tilde p(\\theta_0,v_0| \\theta_t,  v_t)}{\\tilde p( \\theta_t,  v_t|\\theta_0,v_0)}}^1 \\right)$$\n",
    "\n",
    "  where the second proposal distribution $\\tilde q(\\theta_t,  v_t+\\epsilon|\\theta_t,v_t)$ ratio cancels due to the symmetry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18de88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,2*np.pi, 100); \n",
    "fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "ax[0].plot(np.cos(t), -np.sin(t))\n",
    "ax[0].set_aspect('equal', adjustable='box')\n",
    "ax[1].set_aspect('equal', adjustable='box')\n",
    "ax[0].set_title(\"(Second Order Symplectic) Leapfrog Method\")\n",
    "ax[1].set_title(\"Exponentiated Energy Distribution\")\n",
    "ax[2].set_title(\"Marginal Distribution of Interest\")\n",
    "\n",
    "theta_t, v_t, t, epsilon = 0, 1, 0, .1; \n",
    "\n",
    "m = 1000\n",
    "theta_ts = np.zeros(m)\n",
    "v_ts = np.zeros(m)\n",
    "\n",
    "plotting = 50\n",
    "accept = 0\n",
    "for j in range(m):\n",
    "    \n",
    "    v_t_prop = v_t + stats.norm(scale=0.5).rvs()\n",
    "    MH_acceptance_probability = \\\n",
    "    min(1,stats.norm().pdf(v_t_prop)/stats.norm().pdf(v_t))\n",
    "    if stats.uniform().rvs()<MH_acceptance_probability:\n",
    "        v_t = v_t_prop\n",
    "        accept += 1\n",
    "\n",
    "    for i in range(stats.poisson(15).rvs()):\n",
    "        v_t_e = v_t - epsilon / 2 * theta_t\n",
    "        theta_t_e = theta_t + epsilon * v_t_e\n",
    "        v_t_e = v_t_e - epsilon / 2 * theta_t_e\n",
    "        if j < plotting:\n",
    "            ax[0].plot([theta_t, theta_t_e], [v_t, v_t_e], color=['r','k'][i%2]) \n",
    "        theta_t, v_t, t = theta_t_e, v_t_e, t+epsilon\n",
    "    if j < plotting:\n",
    "        ax[0].scatter(theta_t, v_t, s=50, color='orange');\n",
    "        \n",
    "    theta_ts[j] = theta_t\n",
    "    v_ts[j] = v_t\n",
    "        \n",
    "ax[1].scatter(theta_ts, v_ts)\n",
    "ax[2].hist(theta_ts, density=True)\n",
    "ax[2].plot(sorted(theta_ts), stats.norm().pdf(sorted(theta_ts)))\n",
    "accept/m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9a7e8",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte Carlo is just smart Metropolis-Hastings<br>that might actually work in high dimensions<br><sub>Random perturbations in high dimesions just like uniform random samples in high dimensions<br>will not land on the density of interest... high dimensional space is too big...</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f13d3",
   "metadata": {},
   "source": [
    "## High Dimensional Space is Big and Weird and Not What You Think It Is (Part A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e50694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "p_x_pdf = lambda x: stats.norm().pdf(x)\n",
    "mh_tuning_parameter = 1.25\n",
    "q_xt_given_xtm1 = lambda x: stats.norm(x, scale=mh_tuning_parameter)\n",
    "proposal_distribution = q_xt_given_xtm1\n",
    "\n",
    "m = 1000\n",
    "x = np.zeros(m)\n",
    "rejections = 0\n",
    "for t in range(1,m):\n",
    "    x_tilde = proposal_distribution(x[t-1]).rvs()\n",
    "    acceptance_probability = min(1, p_x_pdf(x_tilde)/p_x_pdf(x[t-1]))\n",
    "    if stats.uniform().rvs() < acceptance_probability:\n",
    "        x[t] = x_tilde\n",
    "    else:\n",
    "        x[t] = x[t-1]\n",
    "        rejections += 1\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].plot(x)\n",
    "ax[1].hist(x, density=True)\n",
    "x_support = np.linspace(-4,4,301)\n",
    "ax[1].plot(x_support, p_x_pdf(x_support));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target `stats.multivariate_normal?`\n",
    "D = 20\n",
    "p_x_pdf = lambda x: stats.multivariate_normal(mean=np.zeros(D),cov=np.eye(D)).pdf(x)\n",
    "mh_tuning_parameter = 0.1\n",
    "q_xt_given_xtm1 = lambda x: stats.multivariate_normal(mean=x, cov=mh_tuning_parameter*np.eye(D))\n",
    "proposal_distribution = q_xt_given_xtm1\n",
    "\n",
    "m = 10000\n",
    "x = np.zeros((D,m))\n",
    "rejections = 0\n",
    "for t in range(1,m):\n",
    "    x_tilde = proposal_distribution(x[:,t-1]).rvs()\n",
    "                                    # MVN is still a symmetric proposal\n",
    "    acceptance_probability = min(1, p_x_pdf(x_tilde)/p_x_pdf(x[:,t-1]))  \n",
    "    if stats.uniform().rvs() < acceptance_probability:\n",
    "        x[:,t] = x_tilde\n",
    "    else:\n",
    "        x[:,t] = x[:,t-1]\n",
    "        rejections += 1\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(10,5))\n",
    "\n",
    "demo = 100\n",
    "demo_D = 3\n",
    "for d in range(demo_D):\n",
    "    ax[0,0].plot(x[d,:demo])\n",
    "    ax[0,1].hist(x[d,:], density=True, alpha=0.5)\n",
    "    ax[1,0].plot(x[d,:demo], x[d,1:(demo+1)], '.')\n",
    "    \n",
    "    max_k = 100\n",
    "    autocorrelations = np.ones(max_k)\n",
    "    for k in range(1,max_k):\n",
    "        autocorrelations[k] = np.corrcoef(x[d,:-k:k],x[d,k::k])[0,1]\n",
    "    ax[1,1].plot(autocorrelations, label=\"n_eff = %.2f\"%(m/(1+2*autocorrelations.sum())))\n",
    "    \n",
    "ax[0,0].set_title(\"%.2f\"%(100*rejections/m)+\"% rejections\"+\" (%1.f/\"%rejections+\"%1.f)\"%m)\n",
    "ax[1,1].set_title(\"autocorrelation plots\")\n",
    "ax[1,0].set_title(\"$\\\\theta_t$ versus $\\\\theta_{t+1}$\")\n",
    "ax[1,1].legend()\n",
    "x_support = np.linspace(-4,4,301)\n",
    "ax[0,1].plot(x_support, stats.norm().pdf(x_support))\n",
    "plt.tight_layout()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51534c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1869fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvn_model = pymc.Model()\n",
    "\n",
    "with mvn_model:\n",
    "\n",
    "    target = pymc.MvNormal('target', mu=np.zeros(D), cov=np.eye(D))\n",
    "    \n",
    "m = 10000\n",
    "with mvn_model:\n",
    "\n",
    "    idata = pymc.sample(draws=int(m/4), chains=4, tune=1000)\n",
    "\n",
    "az.plot_trace(idata)\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata, round_to=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5171847",
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = 0,0\n",
    "plt.plot(idata.posterior.target.values[c,:-1,d], \n",
    "         idata.posterior.target.values[c,1:,d], '.', alpha=0.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(idata);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3051f8a",
   "metadata": {},
   "source": [
    "## High Dimensional Space is Big and Weird and Not What You Think It Is (Part B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d14f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a conceptual visualization of what a multivariate normal\n",
    "# distribution is going to look like in high dimensions: a \"bubble\"\n",
    "x_grid = np.linspace(-3,3,1000)\n",
    "x,y = np.meshgrid(x_grid,x_grid)\n",
    "tmp = (x**2+y**2)**.5*stats.multivariate_normal(mean=[0,0]).pdf(np.array([x.ravel(),y.ravel()]).T).reshape(1000,1000)\n",
    "plt.imshow(tmp)\n",
    "plt.xticks(np.linspace(0,1000,7), np.linspace(-3,3,7)); plt.yticks(np.linspace(0,1000,7), np.linspace(-3,3,7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that the \"mass\" of a \"mound\" shape in higher dimensions is actually spherical by\n",
    "# looking at the distribution of distances from the mean of the sample vectors (x**2).sum()**0.5\n",
    "plt.figure()\n",
    "n = 100000\n",
    "for D in [2,5,10,15,20]:\n",
    "    f = stats.multivariate_normal([0]*D)\n",
    "    x = f.rvs(n)\n",
    "    plt.hist((x**2).sum(axis=1)**0.5, density=True, alpha=0.5,\n",
    "             label=\"D=\"+str(D)+\" with average distances \"+str(round(((x**2).sum(axis=1)**0.5).mean(),3)))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f313752",
   "metadata": {},
   "source": [
    "### The MAP an MLE<br><sub></sub>can be bad estimators\n",
    "\n",
    "If the MLE estimates the expectation of a distribution I take it back (and sorry for being so, ahem, **MEAN** to your favorite estimator gosh geez) \n",
    "\n",
    "But consider that the posterior (parameter belief model) distribution is going to be some sort of spherical \"bubble\" shape distribution\n",
    "- since the posterior generally has a \"mound\" shape\n",
    "\n",
    "and that the MAP estimator is not remotely \"usual\" or \"typical\" relative to the **typical set** (which looks like a \"bubble\")\n",
    "- and the MAP estimator is not goign to be \"in the middle or center\" of the \"bubble\" unless the \"bubble\" is perfectly sphereically symmetric (which might be the case for parameters like $\\theta$ but how about for parameters like $\\tau$?)\n",
    "\n",
    "### Posterior expectation is a good estimator<br>averaging over the plausible values according to our (parameter belief) uncertainty<br>so our predictions are based on \"the average\" of what's \"usual\" or \"typical\"\n",
    "\n",
    "$$\\Large E[\\theta|x] = \\int \\theta p(\\theta|x)d\\theta \\approx \\frac{1}{m} \\sum_{t=1}^m \\theta^{(t)} \\quad \\textrm{ for } \\quad \\theta^{(t)}\\sim p(\\theta|x)$$\n",
    "\n",
    "The **posterior predictive distribution** (shown here for a normal distribution data model) is \n",
    "\n",
    "$$\\LARGE f(\\tilde x|x) = \\int N(\\tilde x|\\theta,\\tau) p(\\theta,\\tau|x)d\\theta d\\tau$$\n",
    "\n",
    "\n",
    "with \n",
    "\n",
    "$$\\Large E_{f(\\tilde X|x)}[\\tilde x] = \\frac{1}{m} \\sum_{t=1}^m  \\tilde x^{(t)} \\quad \\textrm{ for } \\quad \\tilde x^{(t)} \\sim N(\\theta^{(t)},\\tau^{(t)}) \\quad \\textrm{ for } \\quad  (\\theta^{(t)}, \\tau^{(t)}) \\sim p(\\beta, \\tau|x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2a67c",
   "metadata": {},
   "source": [
    "## Now that you know<br><sup>High Dimensional Space is Big and Weird and Not What You Think It Is</sup><br>but know that it needs to be treated with care let's do<br>High Dimensional Space is Big <span style=\"color:gray\">and Weird and Not What You Think It Is</span> (Part C): <u>runtime and divergences</u>\n",
    "\n",
    "The diagnostics we have seen to be available so far are\n",
    "\n",
    "- Rejection rate\n",
    "- Effective sample size\n",
    "- Energy\n",
    "\n",
    "but there are two more that are extremely useful\n",
    "\n",
    "- Runtime\n",
    "    - The **step size** of our **symplectic integrator**, and the number of **discrete approximation** steps we take\n",
    "        - determines how far away from our current value we can travel\n",
    "            - and hence determines how much autocorrelation we will have in our Markov chains\n",
    "    - The \"tuning\" that PyMC NUTS algorithm does is to determine the **step size** and number of **discrete approximation** steps the most effectively achieves a **targetted acceptance rate** \n",
    "        - with a default target of an 80% acceptance rate is thought to be a good rule of thumb\n",
    "    - The PyMC NUTS algorithm is interesting in that \"the harder the problem\" the\n",
    "        - the SMALLER **step size** and the GREATER the number of **discrete approximation** steps it requires\n",
    "            - and hence the more computation it requires\n",
    "                - so the slower the run time the \"the harder the problem\" is\n",
    "- Divergences\n",
    "    - The **symplectic integration** is \"falling off the contour\" \n",
    "        - which is easy to do because, remember, high dimensional space is big\n",
    "            - so even very small **discrete approximation** steps might be too big...\n",
    "        - so SMALLER **step size** are required and a GREATER the number of **discrete approximation** steps are required...\n",
    "    - And/But this **divergences** issue happens only in areas of the density with so-called **high curvature**\n",
    "        - so the sampler is tuned to work well on most of the density which likely DOES NOT have **high curvature**\n",
    "        - but then it performs poorly and creates many **divergences** when it arrives in parts of the density with **high curvature**\n",
    "        \n",
    "    - If **divergences** due to **high curvature** are common then the algorithm will be tuned to have SMALLER **step size** and a GREATER the number of **discrete approximation** steps, making it have a slower **runtime**\n",
    "    - If **divergences** still exist then a balance has not been achieved (and it may not be possible to achieve) so sampling will remain problematic in **high curvature** regions of the target density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c2079",
   "metadata": {},
   "source": [
    "### Eight Schools\n",
    "\n",
    "[Eight Schools](https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/Diagnosing_biased_Inference_with_Divergences.html#the-eight-schools-model) is the classical example of a hierarchical \"random effects\" Bayesian model.\n",
    "\n",
    "- The 8 schools have different average levels of achievement ($y$)\n",
    "\n",
    "- but also different uncertainty in their estimation ($\\sigma$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 8\n",
    "y = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\n",
    "sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\n",
    "\n",
    "with pymc.Model() as Centered_eight:\n",
    "\n",
    "    # We assume the following prior distributions for our parameters\n",
    "    mu = pymc.Normal(\"mu\", mu=0, sigma=5)\n",
    "    tau = pymc.HalfCauchy(\"tau\", beta=5)\n",
    "\n",
    "    # and we assume the following probabilistic hierarchy\n",
    "    # within the data generating mechanism\n",
    "    theta = pymc.Normal(\"theta\", mu=mu, sigma=tau, shape=J) # theta is length J=8\n",
    "\n",
    "    # Note that the theta density evalution of theta above can change rapidly \n",
    "    # as a function of tau  meaning that the energy has \"high curvature\"\n",
    "\n",
    "    # likelihood where observed information enters into the model\n",
    "    obs = pymc.Normal(\"obs\", mu=theta, sigma=sigma, observed=y)\n",
    "    \n",
    "pymc.model_to_graphviz(Centered_eight)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd235c9",
   "metadata": {},
   "source": [
    "> This hierarchy seeks to estimate \"true levels of achievement\" $\\theta_i$ given ($\\sigma_i$) noisy observation $y_i$ assuming that there's some variabiltiy over $\\theta_i$ but all come from the same (normal) \"random effects\" distribution with unknown mean and variance.\n",
    ">\n",
    "> Inference will consist of posterior distributions for $\\theta_i$ (and a normal distributional characterization of these 8 values given by posterior distributions over $\\mu$ and $\\tau$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Centered_eight:\n",
    "    fit_C = pymc.sample(5000, chains=2, tune=1000, idata_kwargs={\"log_likelihood\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = fit_C.posterior.tau.values.ravel()\n",
    "diverging = fit_C.sample_stats.diverging.values.ravel()\n",
    "\n",
    "plt.plot(tau[:-1], tau[1:], 'k.', alpha=0.1)\n",
    "divcount = 0\n",
    "for i,div in enumerate(diverging[:-1]):\n",
    "    if div:\n",
    "        if divcount==0:\n",
    "            plt.plot(tau[i], tau[i+1], 'r.', alpha=0.5, label=\"Divergences\")\n",
    "            divcount += 1\n",
    "        plt.plot(tau[i], tau[i+1], 'r.', alpha=0.5)\n",
    "plt.xlabel(\"tau[t]\")\n",
    "plt.ylabel(\"tau[t+1]\")\n",
    "#plt.xlim([0,5])\n",
    "#plt.ylim([0,5])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839654c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "gra = cm.get_cmap('viridis', 100)\n",
    "\n",
    "a = fit_C.sample_stats.acceptance_rate.values.ravel()\n",
    "cutoff=0.4\n",
    "plt.scatter(tau[:-1][a[:-1]>cutoff], tau[1:][a[:-1]>cutoff], \n",
    "            color=gra(a[:-1][a[:-1]>cutoff]), alpha=0.5)\n",
    "for i,ai in enumerate(a[:-1]):\n",
    "    if ai<=cutoff:\n",
    "        plt.scatter(tau[i], tau[i+1], color=gra(ai))\n",
    "\n",
    "plt.scatter(tau[:-1][a[:-1]<=cutoff], tau[1:][a[:-1]<=cutoff], color=gra(a[:-1][a[:-1]<=cutoff]))\n",
    "plt.title(\"Acceptance Rate\")\n",
    "plt.xlabel(\"tau[t]\")\n",
    "plt.ylabel(\"tau[t+1]\")\n",
    "plt.colorbar();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tau, 'k.', alpha=0.1)\n",
    "divcount = 0\n",
    "for i,div in enumerate(diverging):\n",
    "    if div:\n",
    "        if divcount==0:\n",
    "            plt.plot(i,tau[i], 'r.', label=\"Divergences\")\n",
    "            divcount += 1\n",
    "        plt.plot(i, tau[i], 'r.',)\n",
    "plt.title('tau draws')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a526034",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_C.sample_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,3,figsize=(10,10))\n",
    "ax = ax.ravel()\n",
    "look_at = np.array(list(fit_C.sample_stats.keys()))\n",
    "look_at = look_at[[0,2,3,7,12,14]]\n",
    "for i,info in enumerate(look_at):\n",
    "    ax[i].hist(fit_C.sample_stats[info].values.ravel()[~diverging], \n",
    "               density=True, alpha=0.5, label='No divergence')\n",
    "    ax[i].hist(fit_C.sample_stats[info].values.ravel()[diverging], \n",
    "               density=True, alpha=0.5, label='Divergence')\n",
    "    ax[i].set_title(info)\n",
    "    ax[i].legend()\n",
    "\n",
    "ax[-2].hist(fit_C.log_likelihood.obs.values.sum(axis=-1).ravel()[~diverging], \n",
    "            density=True, alpha=0.5, label='No divergence')\n",
    "ax[-2].hist(fit_C.log_likelihood.obs.values.sum(axis=-1).ravel()[diverging], \n",
    "            density=True, alpha=0.5, label='Divergence')\n",
    "ax[-2].set_title(\"log_likelihood\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff24dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = fit_C.posterior.mu.values.ravel()\n",
    "plt.plot(mu, 'k.', alpha=0.1)\n",
    "divcount = 0\n",
    "for i,div in enumerate(diverging):\n",
    "    if div:\n",
    "        if divcount==0:\n",
    "            plt.plot(i,mu[i], 'r.', label=\"Divergences\")\n",
    "            divcount += 1\n",
    "        plt.plot(i, mu[i], 'r.',)\n",
    "        \n",
    "plt.title('mu draws')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,4,figsize=(20,10))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for j in range(J):\n",
    "    theta_j = fit_C.posterior.theta.values[:,:,j].ravel()\n",
    "    ax[j].plot(theta_j, 'k.', alpha=0.1)\n",
    "    divcount = 0\n",
    "    for i,div in enumerate(diverging):\n",
    "        if div:\n",
    "            if divcount==0:\n",
    "                ax[j].plot(i,theta_j[i], 'r.', label=\"Divergences\")\n",
    "                divcount += 1\n",
    "            ax[j].plot(i, theta_j[i], 'r.',)\n",
    "            \n",
    "    ax[j].set_title('theta '+str(j)+' draws')        \n",
    "    ax[j].legend()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ddc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_prec = 1/fit_C.posterior.theta.values.var(axis=-1).ravel()\n",
    "plt.plot(theta_prec, 'k.', alpha=0.1)\n",
    "divcount = 0\n",
    "for i,div in enumerate(diverging):\n",
    "    if div:\n",
    "        if divcount==0:\n",
    "            plt.plot(i,theta_prec[i], 'r.', label=\"Divergences\")\n",
    "            divcount += 1\n",
    "        plt.plot(i, theta_prec[i], 'r.',)\n",
    "\n",
    "plt.title('precision of theta draws')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tau,theta_prec,'k.',alpha=0.05)\n",
    "plt.plot(tau[diverging],theta_prec[diverging], 'r.', label=\"Divergences\", alpha=0.25)\n",
    "plt.xlabel('tau')\n",
    "plt.ylabel('precision of thetas');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = fit_C.posterior.mu.values.ravel()\n",
    "thetaS = fit_C.posterior.theta.values.reshape(10000,8)\n",
    "theta = fit_C.posterior.theta.values\n",
    "for i in range(0,len(mu)-1):\n",
    "    if diverging[i]:\n",
    "        s = max(tau[i]**-0.5, thetaS[i,:].std())\n",
    "        support = np.linspace(mu[i]-2*s, mu[i]+2*s, 100)\n",
    "        plt.plot(support,stats.norm(mu[i],tau[i]**-0.5).pdf(support), label=\"What tau wants\")\n",
    "        plt.plot(support,stats.norm(mu[i],thetaS[i,:].std()).pdf(support), ':', label=\"What thetaS want\")\n",
    "        plt.plot(thetaS[i,:],0*thetaS[i,:],'k.')\n",
    "        #for j in range(J):\n",
    "        #    plt.plot(theta[:,:,j].ravel()[i],0,'r.')\n",
    "        print(i)\n",
    "        plt.legend()\n",
    "        plt.title(\"High curvature as small changes in tau rapidly change energy (density)\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70595d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The many divergences observed above are primarily happening when the \n",
    "# thetas have high precision around mu but are in disagreement with tau\n",
    "# \n",
    "# Changes in tau may therefore result in rapid changes of the likelihood\n",
    "# which means there is \"high curature\" in the energy\n",
    "# and this is resulting in the discrete approximation to the Hamiltonian dynamics \n",
    "# not being accurate enough to correctly walk along a energy contour\n",
    "# so the algorith must slow down by shortening it's step size\n",
    "#\n",
    "# But in this case we can essentially fix this problem with a reparameterization \n",
    "\n",
    "with pymc.Model() as NonCentered_eight:\n",
    "\n",
    "    # The following is a mathematically equivalent specification \n",
    "    # that \"reparameterization our problems away\"\n",
    "    \n",
    "    # We assume the same prior distributions as before\n",
    "    mu = pymc.Normal(\"mu\", mu=0, sigma=5)\n",
    "    tau = pymc.HalfCauchy(\"tau\", beta=5)\n",
    "    \n",
    "    # But now we put theta on a fixed scale independent of tau\n",
    "    theta_tilde = pymc.Normal(\"theta_tilde\", mu=0, sigma=1, shape=J)\n",
    "    # so theta and tau no longer conflict and lead to high energy curvatuare \n",
    "    \n",
    "    # But we can get back our original theta model specification as \n",
    "    theta = pymc.Deterministic(\"theta\", mu + tau * theta_tilde)\n",
    "\n",
    "    # and our likelihood remains exactly the same as before\n",
    "    obs = pymc.Normal(\"obs\", mu=theta, sigma=sigma, observed=y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = [20100420, 20134234] # random seeds for each of two run HMC chains\n",
    "# random_seed=SEED can be passed to `pymc.sample` if desired\n",
    "with NonCentered_eight:\n",
    "    fit_C = pymc.sample(5000, chains=2, tune=1000, idata_kwargs={\"log_likelihood\": True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed88fea",
   "metadata": {},
   "source": [
    "## Week 6 Homework\n",
    "\n",
    "---\n",
    "\n",
    "### Q1: Let's start having some *real* fun...\n",
    "\n",
    "We previously considered the normal-gamma specification \n",
    "\n",
    "$$\\scriptsize\n",
    "\\begin{align*}\n",
    "p(\\theta,\\tau|x) &\\propto{} p(\\theta,\\tau,x) = p(x|\\theta)p(\\theta)p(\\tau) \\quad (\\theta \\perp\\!\\!\\perp \\tau) \\leftarrow \\text{independent priors} & p(\\theta|x,\\theta_0,\\tau_0, \\tau) &={} \\text{N}\\left(\\frac{\\left(\\tau_0 \\theta_0+\\tau\\sum_{i=1}^{n}x_{i}\\right)}{(\\tau_0+n\\tau)}, \\sigma^{-2}=\\tau_0+n\\tau \\right)\\\\\n",
    "&={}  \\left[\\prod_{i=1}^n\\sqrt{\\frac{\\tau}{2\\pi}} e^{-\\frac{\\tau\\left(x_i-\\theta\\right)^2}{2}}\\right] \\sqrt{\\frac{\\tau_0}{2\\pi}} e^{-\\frac{\\tau_0\\left(\\theta-\\theta_0\\right)^2}{2}} \\frac{\\beta ^{\\alpha}}{\\Gamma(\\alpha)} \\tau^{\\alpha -1}e^{-\\beta \\tau} & p(\\tau|x, \\alpha, \\beta, \\theta) &={} \\text{Gamma}\\left(\\frac{\\alpha}{2}+\\frac{n}{2}, \\frac{\\beta}{2}+\\frac{1}{2}\\sum_{i=1}^n\\left(x_i-\\theta\\right)^2 \\right)\\\\{}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "How about instead we consider a \"[location-scale-t](https://en.wikipedia.org/wiki/Student%27s_t-distribution#Location-scale_t-distribution)-norm-halfnorm-discrete-uniform\" specification?\n",
    "\n",
    "$$\\large\n",
    "\\overset{x_i\\; \\sim\\; \\text{location-scale-t}(\\mu, \\sigma^2, \\nu)}{\\quad\\quad\\quad p(x|\\mu,\\sigma^2, \\nu)} = {\\prod_{i=1}^n\n",
    "\\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right) \\sqrt{\\pi \\nu \\sigma^2}}\\left(1+\\frac{1}{\\nu} \\frac{(x_i-\\mu)^2}{\\sigma^2}\\right)^{-(\\nu+1) / 2}}$$\n",
    "\n",
    "$$\\scriptsize \n",
    "\\begin{align}\n",
    "p(\\mu | \\mu_0, \\tau_0) &={} \\sqrt{\\frac{\\tau_0}{2\\pi}} e^{-\\frac{\\tau_0}{2}\\left(\\mu-\\mu_0\\right)^2} & p(\\sigma^2 | \\sigma_0^2) &={} \\sqrt{\\frac{2}{\\pi\\sigma_0^2}} \\exp \\left(-\\frac{(\\sigma^2)^2}{2 \\sigma_0^2}\\right) 1_{[0,\\infty]}(\\sigma^2) & p(\\nu=i) &={} \\Bigg\\{ \\begin{array}{cl} \\frac{1}{100} & \\text{for }i=1,\\cdots,100\\\\ 0 & \\text{otherwise} \\end{array}\\\\\n",
    "& \\textrm{normal} && \\textrm{half-normal} && \\textrm{discrete uniform}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Um yeah we're gonna need a Metroposlis cleanup on aisles one two and three  \n",
    "(or a slice or adapative squeeze rejection sampling steps... in place of Metroposlis steps)\n",
    "\n",
    "*Implement the a Metroposlis within Gibbs algorithm to smaple from the posterior of the above specification. Use a \"smallish\" sample size, say $n=100$ and implement your acceptance probability on a log-scale as described in [piazza post @65_f1](https://piazza.com/class/m5jvyco84083fm/post/65_f1)*\n",
    "\n",
    "### Q2: explore the role of sample size in providing inference for the degrees of freedom parameter $\\nu$\n",
    "\n",
    "*Implement the specification above using `PyMC` where you can explore inference on $\\nu$ at different sample sizes. Provide a summarization and explanation of your findings.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b3612",
   "metadata": {},
   "source": [
    "### Q3: the posterior predictive distribution does something like the following...\n",
    "\n",
    "Let $p(\\tau_i)$ be $\\require{cancel} \\textrm{gamma}\\big(\\tau_i | \\alpha = \\frac{\\nu}{2}, \\overset{\\textrm{rate}\\xcancel{\\textrm{scale}}}{\\beta = \\frac{\\nu}{2}}\\big)$ and let $p(y_i|\\tau_i)$ be $\\textrm{N}(y_i | 0,\\tau_i)$ and now integrate out the uncertainty in $\\tau_i$ and see what distribution is left over for $y_i$.\n",
    "\n",
    "*Go look at the gamma distribution and remember that you know that the integrals of unnormalized densities are the inverse of their normalizing constants. Then go look at the t distribution and determine what distribution the following expression defines. Then explain why the behavior demonstrated here is analagous to that of the posterior predictive distribution.*\n",
    "\n",
    "$$\\int p(y_i|\\tau_i) p(\\tau_i)  d\\tau_i = \\int \\sqrt{\\frac{\\tau_i}{2\\pi}}e^{-\\frac{1}{2}\\tau_i y_i^2} \\frac {\\frac{\\nu}{2}^{\\frac{\\nu}{2}}}{\\Gamma \\left(\\frac{\\nu}{2}\\right)} \\tau_i^{\\frac{\\nu}{2}-1}e^{-\\frac{\\nu}{2}\\tau_i} d\\tau_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd68087",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cff2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
